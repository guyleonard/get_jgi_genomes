#!/usr/bin/env perl
use strict;
use warnings;

use Cwd;
use File::Basename;
use File::Find::Rule;
use File::Path qw(make_path);
use Getopt::Std;
use List::MoreUtils qw(uniq);
use open qw(:std :utf8);
use utf8;
use XML::LibXML;

use Data::Dumper;

my $cookies = 'cookies';
my ( $username, $password, $outdir, $project );

my %options = ();
getopts( 'u:p:c:g:o:lh', \%options ) or display_help();

# Help
if ( $options{h} ) { display_help(); }

# Signin Options / Cookies
if ( defined $options{u} && defined $options{p} ) {
    $username = "$options{u}";
    $password = "$options{p}";
    signin( $username, $password );
}
elsif ( defined $options{c} ) {
    $cookies = $options{c};
    print "User supplied cookie, skipping signin process.\n";
}
else {
    display_help();
}

# Download Project XML & Output Dir
if ( defined $options{o} && defined $options{g} ) {
    $outdir  = "$options{o}";
    $project = "$options{g}";

    print "Downloading XML from JGI Project: $project\n";
    download_xml($project);
}
else {
    print "No Output Directory or Group Project ID\n";
    exit(1);
}

print "Parsing XML\n";
my $list = "false";
if ( $options{l} ) {
    $list = "true";
    print "\tOutput: List Only\n";
    parse_xml( $project, $outdir, $list );
}
else {
    parse_xml( $project, $outdir, $list );
}

sub display_help {
    print "Usage:\n";
    print
        "  get_jgi_genomes.pl [-u <username> -p <password>] | [-c <cookies>] -g <portal> -o <outdir> (-l)\n\n";
    print "Required:\n";
    print "  -u <username>\n";
    print "  -p <password>\n";
    print "or\n";
    print "  -c <cookie file>\n";
    print "and\n";
    print "  -g <project> (e.g. fungi)\n";
    print "---\n";
    print "Optional:\n";
    print "  -l list individual projects only to file (no downloads)\n";

    #print "\t-x xml file\n";
    exit(1);
}

# JGI insist we log in, that's okay. You should be aware that different genome portals
# may have different licenses etc...it's up to the individual user to be aware of these.
# So, let's login to the DOE JGI SSO
# We can do this with curl and our username/password
# This saves a cookie file for use later, it will need refreshing.
sub signin {

    my $user = shift;
    my $pass = shift;

    if ( -e 'cookies' ) {
        if ( -A 'cookies' > 1 ) {
            print "Logging In Again...\n";
            run_cmd(
                "curl --silent 'https://signon-old.jgi.doe.gov/signon/create' --data-urlencode 'login=$user' --data-urlencode 'password=$pass' -c cookies > /dev/null"
            );
            print "Successfully Logged In!\n";
        }
        else {
            print "Already logged in...\n";
        }
    }
    else {
        print "Logging In...\n";
        run_cmd(
            "curl --silent 'https://signon-old.jgi.doe.gov/signon/create' --data-urlencode 'login=$user' --data-urlencode 'password=$pass' -c cookies > /dev/null"
        );
        print "Successfully Logged In!\n";
    }
}

# Now let's use curl again with our cookie signon and scrape the XML
# Only refresh if the file is older than 10 days...You can change this as you like
sub download_xml {
    my $portal = shift;

    if ( !-e "$portal\_files.xml" ) {    #> 10 ) {

        # Get portal List
        print "Downloading $portal XML - This may take some time...\n";
        run_cmd(
            "curl 'https://genome.jgi.doe.gov/portal/ext-api/downloads/get-directory?organism=$portal' -b $cookies > $portal\_files.xml"
        );
    }
    else {
        print
            "\t$portal\_files.xml has not been modified in > 10 days, skipping download.\n";
    }

    # I can't get the XML parsing to work when "&quot;" exists in the file
    # let's cheat and remove it with sed?
    run_cmd("sed -i \'s/&quot;//g\' $portal\_files.xml");
}

## Parse the XML DOM
sub parse_xml {

    my ( $portal, $outdir, $list ) = @_;

    my $xml_file = "$portal\_files.xml";

    my $portal_xml
        = XML::LibXML->load_xml( location => $xml_file, no_blanks => 1 );

    my $query
        = '/organismDownloads[@name="fungi"]/folder[@name="Files"]/folder[@name="Annotation"]/folder[@name="Filtered Models (best)"]/folder[@name="Proteins"]';

    my %results;

    for my $file ( $portal_xml->findnodes($query) ) {
        for my $file_props ( $file->findnodes('./*') ) {

            my $label = $file_props->getAttribute('label');
            my $url   = $file_props->getAttribute('url');

            # if the files don't look like this, then we don't want them
            # Armlut1_GeneCatalog_proteins_20180531.aa.fasta.gz
            # *proteins.fasta.gz
            # we also remove any files with some modifiers
            # then push them to a hash to remove genomes with two sets
            # of proteins.
            # it's a bit of a fudge, but the xml makes no effort to define
            # the set of AAs that are representative of the genome

            if ( $url =~ m/.*\_GeneCatalog_proteins_\d+\.aa\.fasta.gz/ ) {
                if ( $url !~ m/primary|secondary|alleles|diploid|old/ ) {
                    $results{"$label"} = "$url";
                }
            }
            elsif ( $url =~ m/.*proteins\.fasta\.gz/ ) {
                $results{"$label"} = "$url";
            }
        }
    }

    if ( !-d $outdir ) {
        make_path($outdir);
    }

    if ( $list eq "true" ) {

        genome_list( \%results, $outdir, $portal );
    }

    #     else {
    #         #download_files( \@urls, $outdir );
    #     }
    #}
}

sub genome_list {
    my %list_of_urls = %{ $_[0] };
    my $outdir       = $_[1];
    my $portal       = $_[2];

    my $filename = "$outdir\/$portal\_list.txt";
    open my $fileout, '>>', $filename;

    foreach ( sort keys %list_of_urls ) {
        my $taxa   = $_;
        my $url    = $list_of_urls{$_};
        my @jgi_id = split /\//, $url;

# URL from XML
# portal/Zymps1/download/Zymps1_GeneCatalog_proteins_20141012.aa.fasta.gz
# Actual JGI download URL
# https://genome.jgi.doe.gov/portal/Aaoar1/download/Aaoar1_GeneCatalog_proteins_20140429.aa.fasta.gz
        print $fileout
            "$taxa\t$jgi_id[2]\thttps://genome.jgi.doe.gov/portal/$jgi_id[2]/download/$jgi_id[4]\n";
    }

    close($fileout);
}

sub download_files {
    my @urls   = @{ $_[0] };
    my $outdir = $_[1];

    foreach my $current (@urls) {
        my ( $file, $dir, $ext ) = fileparse( $current, '\.gz' );
        my @jgi_id = split /\//, $dir;
        my $taxa = "$jgi_id[2]";

        my @file_match
            = File::Find::Rule->file()->name("$file$ext")->in("$outdir");

        if ( grep( /$file$ext/, @file_match ) ) {
            print "\t\tSkipping: $file$ext Exists\n";
        }
        else {
            print "\tRetrieving: $file\n";
            if ( $file =~ /gff/igs ) {
                run_cmd(
                    "curl --silent 'https://genome.jgi.doe.gov/portal/$taxa/download/$file$ext' -b cookies > $outdir\/gff\/$file$ext"
                );
            }
            elsif ( $file =~ /alleles/igs ) {
                run_cmd(
                    "curl --silent 'https://genome.jgi.doe.gov/portal/$taxa/download/$file$ext' -b cookies > $outdir\/alleles\/$file$ext"
                );
            }
            elsif ( $file =~ /tab/igs ) {
                run_cmd(
                    "curl --silent 'https://genome.jgi.doe.gov/portal/$taxa/download/$file$ext' -b cookies > $outdir\/tab\/$file$ext"
                );
            }
            else {
                run_cmd(
                    "curl --silent 'https://genome.jgi.doe.gov/portal/$taxa/download/$file$ext' -b cookies > $outdir\/fasta\/$file$ext"
                );
            }
        }
    }
}

sub run_cmd {
    my ( $cmd, $quiet ) = @_;
    msg("Running: $cmd") unless $quiet;
    system($cmd) == 0 or error("Error $? running command");
}

sub error {
    msg(@_);
    exit(1);
}

sub msg {
    print STDERR "@_\n";
}
